<HTML>
<P><A HREF = "Manual.html">fftMPI documentation</A>
</P>
<H3>Introduction 
</H3>
<P>The fftMPI library computes multi-dimensional FFTs in parallel where
the FFT grid is distributed across processors.  Features and
limitations of the library are as follows:
</P>
<UL><LI>2d or 3d FFTs
<LI>complex-to-complex FFTs
<LI>single or double precision
<LI>compute FFTs in place, or output to separate memory
<LI>runs on any number of processors, including 1 proc
<LI>allowed grid size in each dimension = any product of 2,3,5 prime factors
<LI>grid decomposition = arbitrary tiling across MPI tasks (explained below)
<LI>initial/final decompositions of grid can be different (useful for convolutions)
<LI>auto-tuning option for a few parameters which affect performance
<LI>1d FFTs computed by an external library: <A HREF = "http://www.fftw.org">FFTW</A>, <A HREF = "mkl">MKL</A>, or <A HREF = "https://sourceforge.net/projects/kissfft">KISS</A>
<LI>data movement/reordering methods can be used without FFTs
<LI>invoke multiple instances of the library (e.g. with MPI sub-communicators)
<LI>callable from C++ or any language via a C interface (e.g. C, Fortran, Python)
<LI>test programs and interface files included for all 4 of these languages
<LI>CPU only execution, currently no OpenMP or GPU support 
</UL>
<P>In the fftMPI context, a "tiling" of the 2d or 3d FFT grid is how it
is distributed across processors.  Imagine a N1 x N2 or N1 x N2 x N3
grid partitioned into P tiles, where P is the number of MPI tasks
(processors).  Each tile is a "rectangle" of grid points in 2d or
"brick" of grid points in 3d.  Each processor's tile can be any size
or shape, including empty.  The P individual tiles cannot overlap;
their union is the entire grid.  This means each point of the global
FFT grid is owned by a unique processor.
</P>
<P>A 2d FFT is performed as a set of N2 1d FFTs in the first dimension,
followed by N1 1d FFTs in the 2nd dimension.  A 3d FFT is performed as
N2*N3 1d FFTs in the first dimension, then N1*N3 1d FFTs in the 2nd,
then N1*N2 1d FFTs in the third dimension.
</P>
<P>The FFT result can overwrite the input values (in-place FFT) or be
written to new memory.  However note that fftMPI also allocates additional
memory internally to buffer MPI send and recv messages.
</P>
<P>The 1d FFTs are not computed by fftMPI itself, but rather by calls
each processor makes to an external library.  As listed above, fftMPI
has support for these libraries:
</P>
<UL><LI><A HREF = "http://www.fftw.org">FFTW (version 3 or 2)</A>
<LI><A HREF = "mkl">Intel MKL</A>
<LI><A HREF = "https://sourceforge.net/projects/kissfft">KISS FFT</A> (provided with fftMPI) 
</UL>






<P>What fftMPI encodes is the parallel communication necessary to remap
grid data between processors.  This involves both sending/receiving
data between processors and re-ordering data on-processor, between
each stage of 1d FFT computations.  This distributes the sets of 1d
FFT computations across processors, and stores the data for individual
1d FFTs contiguously on each processor.
</P>
</HTML>
